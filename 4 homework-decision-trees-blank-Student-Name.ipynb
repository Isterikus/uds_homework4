{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 4: Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we gonna learn how to choose between ML models, based on data type. Your task would be to predict **the edibility of a mushroom** based on sample descriptions (binary classification problem)\n",
    "\n",
    "The **tricky part here is that 95% of the features are of categorical type.**\n",
    "<br>That's the one where we would **(usually)  prefer tree-based algorithms over linear methods**\n",
    "\n",
    "Although this dataset was originally contributed to the UCI Machine Learning repository nearly 30 years ago, mushroom hunting (otherwise known as \"shrooming\") is enjoying new peaks in popularity. Learn which features spell certain death and which are most palatable in this dataset of mushroom characteristics. And how certain can your model be?\n",
    "\n",
    "This dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms (1981). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like \"leaflets three, let it be'' for Poisonous Oak and Ivy.\n",
    "\n",
    "More information can be found [here](https://www.kaggle.com/uciml/mushroom-classification/data)\n",
    "\n",
    "Please find below correspondent [google form](https://docs.google.com/forms/d/e/1FAIpQLScmKfUApMlcD81u9UZxM7xG3vJiEJHrPrG-3b0i_jyPEDijgQ/viewform) to submit your answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:13:18.719050Z",
     "start_time": "2018-03-01T19:13:18.204437Z"
    }
   },
   "outputs": [],
   "source": [
    "# library import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from os.path import join as pjoin\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "# preprocessing / validation\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    ")\n",
    "# ML models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:13:18.765926Z",
     "start_time": "2018-03-01T19:13:18.719050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6499, 23) (1625, 22)\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "DATA_DIR = ''\n",
    "df_train = pd.read_csv('4-mushrooms-train.csv', engine='c')\n",
    "df_test = pd.read_csv('4-mushrooms-test.csv', engine='c')\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:13:18.834937Z",
     "start_time": "2018-03-01T19:13:18.803688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8124, 24)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True     6499\n",
       "False    1625\n",
       "Name: is_train, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for convenient calculations, let us merge train with test\n",
    "df = pd.concat([df_train, df_test], axis=0)\n",
    "# add column for filtering train/test\n",
    "df['is_train'] = True\n",
    "df.loc[df.target.isnull(), 'is_train'] = False\n",
    "# check shapes\n",
    "print(df.shape)\n",
    "# check labels\n",
    "df.is_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bruises</th>\n",
       "      <th>cap_color</th>\n",
       "      <th>cap_shape</th>\n",
       "      <th>cap_surface</th>\n",
       "      <th>gill_attachment</th>\n",
       "      <th>gill_color</th>\n",
       "      <th>gill_size</th>\n",
       "      <th>gill_spacing</th>\n",
       "      <th>habitat</th>\n",
       "      <th>odor</th>\n",
       "      <th>population</th>\n",
       "      <th>ring_number</th>\n",
       "      <th>ring_type</th>\n",
       "      <th>spore_print_color</th>\n",
       "      <th>stalk_color_above_ring</th>\n",
       "      <th>stalk_color_below_ring</th>\n",
       "      <th>stalk_root</th>\n",
       "      <th>stalk_shape</th>\n",
       "      <th>stalk_surface_above_ring</th>\n",
       "      <th>stalk_surface_below_ring</th>\n",
       "      <th>target</th>\n",
       "      <th>veil_color</th>\n",
       "      <th>veil_type</th>\n",
       "      <th>is_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bruises</td>\n",
       "      <td>brown</td>\n",
       "      <td>convex</td>\n",
       "      <td>scaly</td>\n",
       "      <td>free</td>\n",
       "      <td>white</td>\n",
       "      <td>narrow</td>\n",
       "      <td>close</td>\n",
       "      <td>urban</td>\n",
       "      <td>pungent</td>\n",
       "      <td>scattered</td>\n",
       "      <td>one</td>\n",
       "      <td>pendant</td>\n",
       "      <td>brown</td>\n",
       "      <td>white</td>\n",
       "      <td>white</td>\n",
       "      <td>equal</td>\n",
       "      <td>enlarging</td>\n",
       "      <td>smooth</td>\n",
       "      <td>smooth</td>\n",
       "      <td>0.0</td>\n",
       "      <td>white</td>\n",
       "      <td>partial</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bruises</td>\n",
       "      <td>gray</td>\n",
       "      <td>flat</td>\n",
       "      <td>fibrous</td>\n",
       "      <td>free</td>\n",
       "      <td>brown</td>\n",
       "      <td>broad</td>\n",
       "      <td>close</td>\n",
       "      <td>woods</td>\n",
       "      <td>none</td>\n",
       "      <td>several</td>\n",
       "      <td>one</td>\n",
       "      <td>pendant</td>\n",
       "      <td>brown</td>\n",
       "      <td>white</td>\n",
       "      <td>white</td>\n",
       "      <td>bulbous</td>\n",
       "      <td>tapering</td>\n",
       "      <td>smooth</td>\n",
       "      <td>smooth</td>\n",
       "      <td>1.0</td>\n",
       "      <td>white</td>\n",
       "      <td>partial</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no</td>\n",
       "      <td>brown</td>\n",
       "      <td>flat</td>\n",
       "      <td>smooth</td>\n",
       "      <td>attached</td>\n",
       "      <td>orange</td>\n",
       "      <td>broad</td>\n",
       "      <td>close</td>\n",
       "      <td>leaves</td>\n",
       "      <td>none</td>\n",
       "      <td>several</td>\n",
       "      <td>one</td>\n",
       "      <td>pendant</td>\n",
       "      <td>brown</td>\n",
       "      <td>orange</td>\n",
       "      <td>orange</td>\n",
       "      <td>missing</td>\n",
       "      <td>enlarging</td>\n",
       "      <td>smooth</td>\n",
       "      <td>smooth</td>\n",
       "      <td>0.0</td>\n",
       "      <td>orange</td>\n",
       "      <td>partial</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bruises</td>\n",
       "      <td>gray</td>\n",
       "      <td>convex</td>\n",
       "      <td>fibrous</td>\n",
       "      <td>free</td>\n",
       "      <td>brown</td>\n",
       "      <td>broad</td>\n",
       "      <td>close</td>\n",
       "      <td>woods</td>\n",
       "      <td>none</td>\n",
       "      <td>solitary</td>\n",
       "      <td>one</td>\n",
       "      <td>pendant</td>\n",
       "      <td>black</td>\n",
       "      <td>white</td>\n",
       "      <td>white</td>\n",
       "      <td>bulbous</td>\n",
       "      <td>tapering</td>\n",
       "      <td>smooth</td>\n",
       "      <td>smooth</td>\n",
       "      <td>1.0</td>\n",
       "      <td>white</td>\n",
       "      <td>partial</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no</td>\n",
       "      <td>brown</td>\n",
       "      <td>knobbed</td>\n",
       "      <td>smooth</td>\n",
       "      <td>free</td>\n",
       "      <td>buff</td>\n",
       "      <td>narrow</td>\n",
       "      <td>close</td>\n",
       "      <td>paths</td>\n",
       "      <td>foul</td>\n",
       "      <td>several</td>\n",
       "      <td>one</td>\n",
       "      <td>evanescent</td>\n",
       "      <td>white</td>\n",
       "      <td>pink</td>\n",
       "      <td>pink</td>\n",
       "      <td>missing</td>\n",
       "      <td>tapering</td>\n",
       "      <td>silky</td>\n",
       "      <td>smooth</td>\n",
       "      <td>0.0</td>\n",
       "      <td>white</td>\n",
       "      <td>partial</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bruises cap_color cap_shape cap_surface gill_attachment gill_color  \\\n",
       "0  bruises     brown    convex       scaly            free      white   \n",
       "1  bruises      gray      flat     fibrous            free      brown   \n",
       "2       no     brown      flat      smooth        attached     orange   \n",
       "3  bruises      gray    convex     fibrous            free      brown   \n",
       "4       no     brown   knobbed      smooth            free       buff   \n",
       "\n",
       "  gill_size gill_spacing habitat     odor population ring_number   ring_type  \\\n",
       "0    narrow        close   urban  pungent  scattered         one     pendant   \n",
       "1     broad        close   woods     none    several         one     pendant   \n",
       "2     broad        close  leaves     none    several         one     pendant   \n",
       "3     broad        close   woods     none   solitary         one     pendant   \n",
       "4    narrow        close   paths     foul    several         one  evanescent   \n",
       "\n",
       "  spore_print_color stalk_color_above_ring stalk_color_below_ring stalk_root  \\\n",
       "0             brown                  white                  white      equal   \n",
       "1             brown                  white                  white    bulbous   \n",
       "2             brown                 orange                 orange    missing   \n",
       "3             black                  white                  white    bulbous   \n",
       "4             white                   pink                   pink    missing   \n",
       "\n",
       "  stalk_shape stalk_surface_above_ring stalk_surface_below_ring  target  \\\n",
       "0   enlarging                   smooth                   smooth     0.0   \n",
       "1    tapering                   smooth                   smooth     1.0   \n",
       "2   enlarging                   smooth                   smooth     0.0   \n",
       "3    tapering                   smooth                   smooth     1.0   \n",
       "4    tapering                    silky                   smooth     0.0   \n",
       "\n",
       "  veil_color veil_type  is_train  \n",
       "0      white   partial      True  \n",
       "1      white   partial      True  \n",
       "2     orange   partial      True  \n",
       "3      white   partial      True  \n",
       "4      white   partial      True  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T15:42:12.821025Z",
     "start_time": "2018-03-01T15:42:09.355356Z"
    }
   },
   "source": [
    "### Task 1. Which feature has the highest amount of unique values? (joint dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:13:18.866187Z",
     "start_time": "2018-03-01T19:13:18.834937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bruises 2\n",
      "cap_color 10\n",
      "cap_shape 6\n",
      "cap_surface 4\n",
      "gill_attachment 2\n",
      "gill_color 12\n",
      "gill_size 2\n",
      "gill_spacing 2\n",
      "habitat 7\n",
      "odor 9\n",
      "population 6\n",
      "ring_number 3\n",
      "ring_type 5\n",
      "spore_print_color 9\n",
      "stalk_color_above_ring 9\n",
      "stalk_color_below_ring 9\n",
      "stalk_root 5\n",
      "stalk_shape 2\n",
      "stalk_surface_above_ring 4\n",
      "stalk_surface_below_ring 4\n",
      "target 2\n",
      "veil_color 4\n",
      "veil_type 1\n",
      "is_train 2\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "# ---------------------------------------------------------------\n",
    "# most_diversive = ''\n",
    "mn = 0\n",
    "mn_feature = \"\"\n",
    "for feature in df.columns:\n",
    "    print(feature, df[feature].nunique())\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# print(most_diversive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T15:54:52.296735Z",
     "start_time": "2018-03-01T15:54:52.281115Z"
    }
   },
   "source": [
    "### Task 2\n",
    "**As a preparation, one would spend up to 15-30 minutes on exploratory data analysis (EDA)** - make sure you understand how features are distributed in train/test, what they look like, are they ordinal/binary/categorical before moving further\n",
    "<br>While doing it, please answer the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bruises\n",
      " ['bruises' 'no']\n",
      "cap_color\n",
      " ['brown' 'gray' 'red' 'white' 'yellow' 'buff' 'pink' 'green' 'purple'\n",
      " 'cinnamon']\n",
      "cap_shape\n",
      " ['convex' 'flat' 'knobbed' 'bell' 'sunken' 'conical']\n",
      "cap_surface\n",
      " ['scaly' 'fibrous' 'smooth' 'grooves']\n",
      "gill_attachment\n",
      " ['free' 'attached']\n",
      "gill_color\n",
      " ['white' 'brown' 'orange' 'buff' 'pink' 'black' 'purple' 'gray' 'green'\n",
      " 'chocolate' 'yellow' 'red']\n",
      "gill_size\n",
      " ['narrow' 'broad']\n",
      "gill_spacing\n",
      " ['close' 'crowded']\n",
      "habitat\n",
      " ['urban' 'woods' 'leaves' 'paths' 'grasses' 'waste' 'meadows']\n",
      "odor\n",
      " ['pungent' 'none' 'foul' 'fishy' 'spicy' 'anise' 'creosote' 'musty'\n",
      " 'almond']\n",
      "population\n",
      " ['scattered' 'several' 'solitary' 'numerous' 'clustered' 'abundant']\n",
      "ring_number\n",
      " ['one' 'two' 'none']\n",
      "ring_type\n",
      " ['pendant' 'evanescent' 'none' 'large' 'flaring']\n",
      "spore_print_color\n",
      " ['brown' 'black' 'white' 'chocolate' 'green' 'buff' 'yellow' 'orange'\n",
      " 'purple']\n",
      "stalk_color_above_ring\n",
      " ['white' 'orange' 'pink' 'gray' 'cinnamon' 'red' 'brown' 'buff' 'yellow']\n",
      "stalk_color_below_ring\n",
      " ['white' 'orange' 'pink' 'gray' 'cinnamon' 'brown' 'buff' 'red' 'yellow']\n",
      "stalk_root\n",
      " ['equal' 'bulbous' 'missing' 'club' 'rooted']\n",
      "stalk_shape\n",
      " ['enlarging' 'tapering']\n",
      "stalk_surface_above_ring\n",
      " ['smooth' 'silky' 'fibrous' 'scaly']\n",
      "stalk_surface_below_ring\n",
      " ['smooth' 'silky' 'fibrous' 'scaly']\n",
      "target\n",
      " [ 0.  1. nan]\n",
      "veil_color\n",
      " ['white' 'orange' 'brown' 'yellow']\n",
      "veil_type\n",
      " ['partial']\n",
      "is_train\n",
      " [ True False]\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "# ---------------------------------------------------------------\n",
    "# most_diversive = ''\n",
    "mn = 0\n",
    "mn_feature = \"\"\n",
    "for feature in df.columns:\n",
    "    print(feature + \"\\n\", df[feature].unique())\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# print(most_diversive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T16:27:41.214779Z",
     "start_time": "2018-03-01T16:27:41.199155Z"
    }
   },
   "source": [
    "#### 2.1 Are there any features, obviously redundant to train on? If yes - what are they and why it's better to remove them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='ring_number', hue='target', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:13:18.881811Z",
     "start_time": "2018-03-01T19:13:18.866187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8124, 24)\n",
      "(8124, 23)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "# your code/hardcoded list goes here\n",
    "# ---------------------------------------------------------------\n",
    "redundant_columns = [\n",
    "    'veil_type' # has only one unique feature\n",
    "]\n",
    "# ---------------------------------------------------------------\n",
    "# lets drop these columns from joint dataset\n",
    "df = df.drop(redundant_columns, axis=1, errors='ignore')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2.2 How many features (excluding target variable and train/test indexing columns) are:\n",
    "- categorical (more than 2 unique values, no explicit ordering)\n",
    "- ordinal (more than 2 unique values, explicit ordering)\n",
    "- binary (2 unique values, doesn't matter whether it has ordering or is \"yes/no\" styled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:13:18.903953Z",
     "start_time": "2018-03-01T19:13:18.883318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical: 15\n",
      "ordinal: 1\n",
      "binary: 5\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "# ---------------------------------------------------------------\n",
    "ordinal_cols = sorted([\n",
    "    'ring_number', \n",
    "])\n",
    "binary_cols = sorted([\n",
    "    'bruises', 'gill_attachment', 'gill_size', 'gill_spacing',\n",
    "    'stalk_shape'\n",
    "])\n",
    "categorical_cols = sorted([\n",
    "    'cap_color', 'cap_shape', 'cap_surface', 'gill_color',\n",
    "    'habitat', 'odor', 'population', 'ring_type',\n",
    "    'spore_print_color', 'stalk_color_above_ring', 'stalk_color_below_ring',\n",
    "    'stalk_root', 'stalk_surface_above_ring', 'stalk_surface_below_ring', 'veil_color'\n",
    "])\n",
    "# ---------------------------------------------------------------\n",
    "print('categorical: {}\\nordinal: {}\\nbinary: {}'.format(\n",
    "    len(categorical_cols), len(ordinal_cols), len(binary_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:13:19.066710Z",
     "start_time": "2018-03-01T19:13:18.903953Z"
    }
   },
   "outputs": [],
   "source": [
    "# To be used in training, data must be properly encoded\n",
    "from collections import defaultdict\n",
    "\n",
    "# function to encode categorical data\n",
    "\n",
    "\n",
    "def __encode_categorical(df_list, cat_cols):\n",
    "    # initialize placeholder\n",
    "    d = defaultdict(LabelEncoder)\n",
    "    # fit and encode train/test,\n",
    "    codes = pd.concat(\n",
    "        [df[cat_cols] for df in df_list],\n",
    "        axis=0\n",
    "    ).fillna('').apply(\n",
    "        lambda x: d[x.name].fit(x)\n",
    "    ),\n",
    "    # transform encodings to train/test etc\n",
    "    for df in df_list:\n",
    "        df[cat_cols] = df[cat_cols].fillna('').apply(\n",
    "            lambda x: d[x.name].transform(x))\n",
    "\n",
    "\n",
    "# label encode data (categorical + binary)\n",
    "__encode_categorical(df_list=[df], cat_cols=categorical_cols+binary_cols)\n",
    "# make sure you encode the only ordinal column in correct order\n",
    "df[ordinal_cols[0]] = df[ordinal_cols[0]].map({'none': 0, 'one': 1, 'two': 2})\n",
    "\n",
    "# define useful feature columns to be used for training\n",
    "# (union of all columns discussed above)\n",
    "columns_to_use = ordinal_cols + binary_cols + categorical_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T16:57:45.813108Z",
     "start_time": "2018-03-01T16:57:45.797483Z"
    }
   },
   "source": [
    "### Task 3. Prepare cross-validation strategy and perform comparison of 2 baseline models (linear vs tree-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### =====================================================\n",
    "#### Briefly about Validation / Cross-Validation\n",
    "\n",
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but **would fail to predict anything useful on yet-unseen data. This situation is called overfitting**. \n",
    "<br>To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set ```X_test, y_test```. \n",
    "<br>Note that the word “experiment” is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally.\n",
    "\n",
    "When evaluating different settings (“hyperparameters”) for estimators, **there is still a risk of overfitting on the test set** because the parameters can be tweaked until the estimator performs optimally. \n",
    "<br>This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. \n",
    "<br>To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
    "\n",
    "However, by partitioning the available data into three sets, **we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.**\n",
    "\n",
    "A solution to this problem is a procedure called **cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV**. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "- A model is trained using k-1 of the folds as training data;\n",
    "- the resulting model is validated on the remaining part of the data \n",
    "<br>(i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "        \n",
    "<img src=\"https://hsto.org/files/b1d/706/e6c/b1d706e6c9df49c297b6152878a2d03f.png\"/ style=\"width:75%\">\n",
    "\n",
    "The performance measure reported by k-fold cross-validation **is then the average of the values computed in the loop**. \n",
    "<br>This approach can be computationally expensive, but does not waste too much data (as it is the case when fixing an arbitrary test set), which is a major advantage in problem such as inverse inference where the number of samples is very small.\n",
    "\n",
    "\n",
    "Some classification problems can **exhibit a large imbalance in the distribution of the target classes: for instance there could be several times more negative samples than positive samples**. \n",
    "<br>In such cases it is recommended to use **stratified sampling** as implemented in sklearn's StratifiedKFold and StratifiedShuffleSplit to ensure that relative class frequencies is approximately preserved in each train and validation fold.\n",
    "\n",
    "More details about different cross-validation strategies, implemented in sklearn, can be found [here](http://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "### ====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T16:57:45.813108Z",
     "start_time": "2018-03-01T16:57:45.797483Z"
    }
   },
   "source": [
    "Prepare KFold with 5 splits, stratified by target variable, shuffled, with fixed random_state = 42\n",
    "<br>**Don't forget to filter by column 'is_train'!**\n",
    "<br>Fit models on subset of features: [columns_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:13:23.353852Z",
     "start_time": "2018-03-01T19:13:19.066710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT scoring: 0.9012\n",
      "LR scoring: 0.8904\n"
     ]
    }
   ],
   "source": [
    "from os import cpu_count\n",
    "\n",
    "n_jobs = max(cpu_count()-1, 1)\n",
    "# your code goes here\n",
    "# ---------------------------------------------------------------\n",
    "# cross-validation iterator\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# xtrain, ytrain, DataFrame-like\n",
    "xtrain = df[df.is_train].drop(['is_train', 'target'], axis=1)\n",
    "ytrain = df[df.is_train]['target']\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# create Decision Tree with default params, max_depth=3, random_state=42\n",
    "dt = DecisionTreeClassifier(\n",
    "    max_depth=3, random_state=42\n",
    ")\n",
    "\n",
    "# estimate its f1-score with cross-validation (cross_val_score)\n",
    "# your code goes here\n",
    "# ---------------------------------------------------------------\n",
    "scores_dt = cross_val_score(\n",
    "    estimator=dt,\n",
    "    X=xtrain,\n",
    "    y=ytrain,\n",
    "    scoring='f1',\n",
    "    cv=kf, # cross-validation strategy\n",
    "    n_jobs=n_jobs\n",
    ").mean()\n",
    "print('DT scoring: {:.4f}'.format(scores_dt))\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "\n",
    "# create Logistic Regression with default params, random_state=42\n",
    "lr = LogisticRegression(\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# estimate its f1-score with cross-validation\n",
    "# your code goes here\n",
    "# ---------------------------------------------------------------\n",
    "scores_lr = cross_val_score(\n",
    "    estimator=lr,\n",
    "    X=xtrain,\n",
    "    y=ytrain,\n",
    "    scoring='f1',\n",
    "    cv=kf,\n",
    "    n_jobs=n_jobs\n",
    ").mean()\n",
    "print('LR scoring: {:.4f}'.format(scores_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is a score of Linear Regression lower than correspondent one of DT?\n",
    "1. Is everything OK with the data format for linear models? (revision of 2 previous lectures). \n",
    "1. If not, what else you should do to use the data appropriately for linear models?\n",
    "1. Why didn't point 1. affect Decision Tree performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T17:07:09.097412Z",
     "start_time": "2018-03-01T17:07:09.081783Z"
    }
   },
   "source": [
    "1)No<br>\n",
    "2)We should do one hot encoding (OHE)<br>\n",
    "3)Because decision trees is imbalanced:) As I think we have a very good distribution of variables in categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the picture below, some categorical variables are compared as numbers and if they were in a different order everything could be worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(xtrain, ytrain);\n",
    "\n",
    "export_graphviz(dt, feature_names=xtrain.columns, \n",
    "out_file='tree2.dot', filled=True)\n",
    "!dot -Tpng 'tree2.dot' -o 'tree2.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tree2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting dummies\n",
    "Let's try to make an OHE and look at the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = df.copy()\n",
    "for cat in categorical_cols:\n",
    "    dummy_df.drop(cat, axis=1, inplace=True)\n",
    "    dummy_df = pd.concat([dummy_df, pd.get_dummies(df[cat], prefix=cat)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bruises</th>\n",
       "      <th>gill_attachment</th>\n",
       "      <th>gill_size</th>\n",
       "      <th>gill_spacing</th>\n",
       "      <th>ring_number</th>\n",
       "      <th>stalk_shape</th>\n",
       "      <th>target</th>\n",
       "      <th>is_train</th>\n",
       "      <th>cap_color_0</th>\n",
       "      <th>cap_color_1</th>\n",
       "      <th>cap_color_2</th>\n",
       "      <th>cap_color_3</th>\n",
       "      <th>cap_color_4</th>\n",
       "      <th>cap_color_5</th>\n",
       "      <th>cap_color_6</th>\n",
       "      <th>cap_color_7</th>\n",
       "      <th>cap_color_8</th>\n",
       "      <th>cap_color_9</th>\n",
       "      <th>cap_shape_0</th>\n",
       "      <th>cap_shape_1</th>\n",
       "      <th>cap_shape_2</th>\n",
       "      <th>cap_shape_3</th>\n",
       "      <th>cap_shape_4</th>\n",
       "      <th>cap_shape_5</th>\n",
       "      <th>cap_surface_0</th>\n",
       "      <th>...</th>\n",
       "      <th>stalk_color_below_ring_1</th>\n",
       "      <th>stalk_color_below_ring_2</th>\n",
       "      <th>stalk_color_below_ring_3</th>\n",
       "      <th>stalk_color_below_ring_4</th>\n",
       "      <th>stalk_color_below_ring_5</th>\n",
       "      <th>stalk_color_below_ring_6</th>\n",
       "      <th>stalk_color_below_ring_7</th>\n",
       "      <th>stalk_color_below_ring_8</th>\n",
       "      <th>stalk_root_0</th>\n",
       "      <th>stalk_root_1</th>\n",
       "      <th>stalk_root_2</th>\n",
       "      <th>stalk_root_3</th>\n",
       "      <th>stalk_root_4</th>\n",
       "      <th>stalk_surface_above_ring_0</th>\n",
       "      <th>stalk_surface_above_ring_1</th>\n",
       "      <th>stalk_surface_above_ring_2</th>\n",
       "      <th>stalk_surface_above_ring_3</th>\n",
       "      <th>stalk_surface_below_ring_0</th>\n",
       "      <th>stalk_surface_below_ring_1</th>\n",
       "      <th>stalk_surface_below_ring_2</th>\n",
       "      <th>stalk_surface_below_ring_3</th>\n",
       "      <th>veil_color_0</th>\n",
       "      <th>veil_color_1</th>\n",
       "      <th>veil_color_2</th>\n",
       "      <th>veil_color_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bruises  gill_attachment  gill_size  gill_spacing  ring_number  \\\n",
       "0        0                1          1             0            1   \n",
       "1        0                1          0             0            1   \n",
       "2        1                0          0             0            1   \n",
       "3        0                1          0             0            1   \n",
       "4        1                1          1             0            1   \n",
       "\n",
       "   stalk_shape  target  is_train  cap_color_0  cap_color_1  cap_color_2  \\\n",
       "0            0     0.0      True            1            0            0   \n",
       "1            1     1.0      True            0            0            0   \n",
       "2            0     0.0      True            1            0            0   \n",
       "3            1     1.0      True            0            0            0   \n",
       "4            1     0.0      True            1            0            0   \n",
       "\n",
       "   cap_color_3  cap_color_4  cap_color_5  cap_color_6  cap_color_7  \\\n",
       "0            0            0            0            0            0   \n",
       "1            1            0            0            0            0   \n",
       "2            0            0            0            0            0   \n",
       "3            1            0            0            0            0   \n",
       "4            0            0            0            0            0   \n",
       "\n",
       "   cap_color_8  cap_color_9  cap_shape_0  cap_shape_1  cap_shape_2  \\\n",
       "0            0            0            0            0            1   \n",
       "1            0            0            0            0            0   \n",
       "2            0            0            0            0            0   \n",
       "3            0            0            0            0            1   \n",
       "4            0            0            0            0            0   \n",
       "\n",
       "   cap_shape_3  cap_shape_4  cap_shape_5  cap_surface_0      ...       \\\n",
       "0            0            0            0              0      ...        \n",
       "1            1            0            0              1      ...        \n",
       "2            1            0            0              0      ...        \n",
       "3            0            0            0              1      ...        \n",
       "4            0            1            0              0      ...        \n",
       "\n",
       "   stalk_color_below_ring_1  stalk_color_below_ring_2  \\\n",
       "0                         0                         0   \n",
       "1                         0                         0   \n",
       "2                         0                         0   \n",
       "3                         0                         0   \n",
       "4                         0                         0   \n",
       "\n",
       "   stalk_color_below_ring_3  stalk_color_below_ring_4  \\\n",
       "0                         0                         0   \n",
       "1                         0                         0   \n",
       "2                         0                         1   \n",
       "3                         0                         0   \n",
       "4                         0                         0   \n",
       "\n",
       "   stalk_color_below_ring_5  stalk_color_below_ring_6  \\\n",
       "0                         0                         0   \n",
       "1                         0                         0   \n",
       "2                         0                         0   \n",
       "3                         0                         0   \n",
       "4                         1                         0   \n",
       "\n",
       "   stalk_color_below_ring_7  stalk_color_below_ring_8  stalk_root_0  \\\n",
       "0                         1                         0             0   \n",
       "1                         1                         0             1   \n",
       "2                         0                         0             0   \n",
       "3                         1                         0             1   \n",
       "4                         0                         0             0   \n",
       "\n",
       "   stalk_root_1  stalk_root_2  stalk_root_3  stalk_root_4  \\\n",
       "0             0             1             0             0   \n",
       "1             0             0             0             0   \n",
       "2             0             0             1             0   \n",
       "3             0             0             0             0   \n",
       "4             0             0             1             0   \n",
       "\n",
       "   stalk_surface_above_ring_0  stalk_surface_above_ring_1  \\\n",
       "0                           0                           0   \n",
       "1                           0                           0   \n",
       "2                           0                           0   \n",
       "3                           0                           0   \n",
       "4                           0                           0   \n",
       "\n",
       "   stalk_surface_above_ring_2  stalk_surface_above_ring_3  \\\n",
       "0                           0                           1   \n",
       "1                           0                           1   \n",
       "2                           0                           1   \n",
       "3                           0                           1   \n",
       "4                           1                           0   \n",
       "\n",
       "   stalk_surface_below_ring_0  stalk_surface_below_ring_1  \\\n",
       "0                           0                           0   \n",
       "1                           0                           0   \n",
       "2                           0                           0   \n",
       "3                           0                           0   \n",
       "4                           0                           0   \n",
       "\n",
       "   stalk_surface_below_ring_2  stalk_surface_below_ring_3  veil_color_0  \\\n",
       "0                           0                           1             0   \n",
       "1                           0                           1             0   \n",
       "2                           0                           1             0   \n",
       "3                           0                           1             0   \n",
       "4                           0                           1             0   \n",
       "\n",
       "   veil_color_1  veil_color_2  veil_color_3  \n",
       "0             0             1             0  \n",
       "1             0             1             0  \n",
       "2             1             0             0  \n",
       "3             0             1             0  \n",
       "4             0             1             0  \n",
       "\n",
       "[5 rows x 111 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT scoring: 0.9142\n",
      "LR scoring: 0.9282\n"
     ]
    }
   ],
   "source": [
    "from os import cpu_count\n",
    "\n",
    "n_jobs = max(cpu_count()-1, 1)\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "xtrain_dummy = dummy_df[dummy_df.is_train].drop(['is_train', 'target'], axis=1)\n",
    "ytrain_dummy = dummy_df[dummy_df.is_train]['target']\n",
    "\n",
    "dt = DecisionTreeClassifier(\n",
    "    max_depth=3, random_state=42\n",
    ")\n",
    "\n",
    "scores_dt = cross_val_score(\n",
    "    estimator=dt,\n",
    "    X=xtrain_dummy,\n",
    "    y=ytrain_dummy,\n",
    "    scoring='f1',\n",
    "    cv=kf,\n",
    "    n_jobs=n_jobs\n",
    ").mean()\n",
    "print('DT scoring: {:.4f}'.format(scores_dt))\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "scores_lr = cross_val_score(\n",
    "    estimator=lr,\n",
    "    X=xtrain_dummy, # ...\n",
    "    y=ytrain_dummy, # ...\n",
    "    scoring='f1',\n",
    "    cv=kf,\n",
    "    n_jobs=n_jobs\n",
    ").mean()\n",
    "print('LR scoring: {:.4f}'.format(scores_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see now the logistic regression is more precise than the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. Now it's time to do some hyperparam tuning\n",
    "Perform suitable hyperparam tuning using created above cross-validation strategy\n",
    "<br>Main parameters to perform grid-search for:\n",
    "- max_depth (1,2,...None)\n",
    "- min_samples_leaf (1,2,...)\n",
    "- criterion (gini, entropy)\n",
    "- weight (none, balanced)\n",
    "- max_features (sqrt(features), 50%, 75%, all of them, ...)\n",
    "- other params available, see documentation\n",
    "\n",
    "So - use your fantasy for filling-in abovementioned lists\n",
    "\n",
    "You should receive **a gain of 0.01 in f1-score or higher**\n",
    "<br>(current benchmark = +0.0268 gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTreeClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:13:36.468592Z",
     "start_time": "2018-03-01T19:13:23.353852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score best: 0.9283, +0.0271 better than baseline\n",
      "CPU times: user 13min 57s, sys: 47.3 s, total: 14min 44s\n",
      "Wall time: 2h 12min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# your code goes here\n",
    "# ---------------------------------------------------------------\n",
    "# create base model (DT, random state = 42)\n",
    "estimator = DecisionTreeClassifier(\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "max_depth = [i for i in range(3, 20)]\n",
    "max_depth.append(None)\n",
    "# create parameter grid\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': list(range(2, 30, 1)) + [None],\n",
    "    'min_samples_split': np.arange(2, 20, 1),\n",
    "    'min_samples_leaf': np.arange(1, 20, 1),\n",
    "    'max_features': np.arange(1, xtrain.shape[1] + 1, 1),\n",
    "}\n",
    "\n",
    "# create grid search object\n",
    "gs = GridSearchCV(\n",
    "    estimator=estimator,  # base model\n",
    "    param_grid=params,  # params grid to search within\n",
    "    cv=kf,  # cross-validation strategy\n",
    "    error_score=1,  # warnings only\n",
    "    scoring='f1',  # f1-score\n",
    "    # thread count, the higher count - the faster\n",
    "    n_jobs=n_jobs,\n",
    "#     verbose=100,  # messages about performed actions\n",
    ")\n",
    "\n",
    "# perform grid search on TRAIN dataset ('is_train' filtering)\n",
    "gs.fit(\n",
    "    X=xtrain, # ...\n",
    "    y=ytrain, # ...\n",
    ")\n",
    "# -------------------------------------------------------------\n",
    "# extract best score on cross-validation\n",
    "best_score = gs.best_score_\n",
    "# extract the estimator (DT) with best params on cross-validation\n",
    "best_dt = gs.best_estimator_\n",
    "# check gain in f1-score\n",
    "print('f1-score best: {:.4f}, +{:.4f} better than baseline'.format(\n",
    "    best_score, (best_score - scores_dt))\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:13:36.484218Z",
     "start_time": "2018-03-01T19:13:36.468592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=9,\n",
       "            max_features=13, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=8, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look on the best model, compare with the baseline\n",
    "best_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T19:13:36.515467Z",
     "start_time": "2018-03-01T19:13:36.484218Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base on train:   0.9012\n",
      "Base on holdout: 0.8888\n",
      "diff: 0.0125\n",
      "\n",
      "Best on train:   0.9283\n",
      "Best on holdout: 0.9205\n",
      "diff: 0.0078\n"
     ]
    }
   ],
   "source": [
    "# check performance on holdout dataset, unseen before (filter 'is_train' == False)\n",
    "\n",
    "# your code goes here\n",
    "# ---------------------------------------------------------------\n",
    "# appropriate df_test data subset from 'df' dataframe\n",
    "xtest = df[df['is_train'] == False].drop(['is_train', 'target'], axis=1)\n",
    "# fit baseline model 'dt' on xtrain, ytrain (because it's not fitted yet)\n",
    "dt.fit(xtrain, ytrain)\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# baseline model\n",
    "y_true = pd.read_csv(pjoin(DATA_DIR, '4-mushrooms-y_test.csv'))\n",
    "y_pred_baseline = dt.predict(xtest)\n",
    "\n",
    "print('Base on train:   {:.4f}\\nBase on holdout: {:.4f}\\ndiff: {:.4f}'.format(\n",
    "    scores_dt, \n",
    "    f1_score(y_true, y_pred_baseline),\n",
    "    scores_dt - f1_score(y_true, y_pred_baseline)\n",
    "))\n",
    "\n",
    "# best model\n",
    "y_pred_best = best_dt.predict(xtest)\n",
    "\n",
    "print('\\nBest on train:   {:.4f}\\nBest on holdout: {:.4f}\\ndiff: {:.4f}'.format(\n",
    "    best_score, \n",
    "    f1_score(y_true, y_pred_best),\n",
    "    best_score - f1_score(y_true, y_pred_best)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see that \n",
    "<br>**absolute values of f1-score is higher and distance between train|holdout is lower** <br>for **best model** in comparison to **baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to do the same on dummy df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "?DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 41040 candidates, totalling 205200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 566 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=3)]: Done 2966 tasks      | elapsed:   20.5s\n",
      "[Parallel(n_jobs=3)]: Done 6966 tasks      | elapsed:   53.6s\n",
      "[Parallel(n_jobs=3)]: Done 12566 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=3)]: Done 19766 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=3)]: Done 28566 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=3)]: Done 38966 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=3)]: Done 50966 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=3)]: Done 64566 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=3)]: Done 79766 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=3)]: Done 96566 tasks      | elapsed: 18.1min\n",
      "[Parallel(n_jobs=3)]: Done 114966 tasks      | elapsed: 21.2min\n",
      "[Parallel(n_jobs=3)]: Done 134966 tasks      | elapsed: 24.1min\n",
      "[Parallel(n_jobs=3)]: Done 156566 tasks      | elapsed: 27.5min\n",
      "[Parallel(n_jobs=3)]: Done 179766 tasks      | elapsed: 32.2min\n",
      "[Parallel(n_jobs=3)]: Done 204566 tasks      | elapsed: 37.2min\n",
      "[Parallel(n_jobs=3)]: Done 205200 out of 205200 | elapsed: 37.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score best: 0.9283, +0.0141 better than baseline\n",
      "CPU times: user 1min 25s, sys: 11.7 s, total: 1min 37s\n",
      "Wall time: 37min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# your code goes here\n",
    "# ---------------------------------------------------------------\n",
    "# create base model (DT, random state = 42)\n",
    "estimator = DecisionTreeClassifier(\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "max_depth = [i for i in range(3, 20)]\n",
    "max_depth.append(None)\n",
    "# create parameter grid\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': list(range(5, 16, 1)) + [None],\n",
    "    'min_samples_split': np.arange(3, 22, 1),\n",
    "    'min_samples_leaf': np.arange(1, 10, 1),\n",
    "    'max_features': np.arange(.5, 1., 0.05),\n",
    "}\n",
    "\n",
    "# create grid search object\n",
    "gs = GridSearchCV(\n",
    "    estimator=estimator,  # base model\n",
    "    param_grid=params,  # params grid to search within\n",
    "    cv=kf,  # cross-validation strategy\n",
    "    error_score=1,  # warnings only\n",
    "    scoring='f1',  # f1-score\n",
    "    # thread count, the higher count - the faster\n",
    "    n_jobs=n_jobs,\n",
    "    verbose=1,  # messages about performed actions\n",
    ")\n",
    "\n",
    "# perform grid search on TRAIN dataset ('is_train' filtering)\n",
    "gs.fit(\n",
    "    X=xtrain_dummy, # ...\n",
    "    y=ytrain_dummy, # ...\n",
    ")\n",
    "# -------------------------------------------------------------\n",
    "# extract best score on cross-validation\n",
    "best_score = gs.best_score_\n",
    "# extract the estimator (DT) with best params on cross-validation\n",
    "best_dt = gs.best_estimator_\n",
    "# check gain in f1-score\n",
    "print('f1-score best: {:.4f}, +{:.4f} better than baseline'.format(\n",
    "    best_score, (best_score - scores_dt))\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=8,\n",
       "            max_features=0.55, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=2, min_samples_split=19,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=9,\n",
    "            max_features=21, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=2, min_samples_split=13,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
    "            splitter='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base on train:   0.9142\n",
      "Base on holdout: 0.9068\n",
      "diff: 0.0074\n",
      "\n",
      "Best on train:   0.9283\n",
      "Best on holdout: 0.9205\n",
      "diff: 0.0078\n"
     ]
    }
   ],
   "source": [
    "xtest = dummy_df[dummy_df['is_train'] == False].drop(['is_train', 'target'], axis=1)\n",
    "dt.fit(xtrain_dummy, ytrain)\n",
    "\n",
    "y_true = pd.read_csv(pjoin(DATA_DIR, '4-mushrooms-y_test.csv'))\n",
    "y_pred_baseline = dt.predict(xtest)\n",
    "\n",
    "print('Base on train:   {:.4f}\\nBase on holdout: {:.4f}\\ndiff: {:.4f}'.format(\n",
    "    scores_dt, \n",
    "    f1_score(y_true, y_pred_baseline),\n",
    "    scores_dt - f1_score(y_true, y_pred_baseline)\n",
    "))\n",
    "\n",
    "y_pred_best = best_dt.predict(xtest)\n",
    "\n",
    "print('\\nBest on train:   {:.4f}\\nBest on holdout: {:.4f}\\ndiff: {:.4f}'.format(\n",
    "    best_score, \n",
    "    f1_score(y_true, y_pred_best),\n",
    "    best_score - f1_score(y_true, y_pred_best)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus question**:\n",
    "\n",
    "Consider two possibilities:\n",
    "- (a) you have trained **one best** (on cross-validation) Decision Tree\n",
    "- (b) you randomly choose 25 subsets of 70% of training data, fits \"overfitted\" (max_depth=None) Decision Trees on it - each of them performs slightly worse than Tree in (a), and then average predicted results over all 25 models (overfitted trees)\n",
    "\n",
    "**Which one of them would most likely give the best results on hold-out dataset? What makes you think that way?**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T17:07:09.097412Z",
     "start_time": "2018-03-01T17:07:09.081783Z"
    }
   },
   "source": [
    "I think option b. When we train one tree because of the problem of retraining, we can trim important branches that in some cases could give a good answer. And we do not have such problems with the forest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
